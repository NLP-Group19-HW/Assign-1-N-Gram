{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading/Preprocessing\n",
        "\n",
        "Upload the `A1_DATASET.zip` file from eLearning to the files.\n",
        "\n",
        "## Preprocessing Explaination\n",
        "\n",
        "- Keep alphabetic characters (convert all to lowercase)\n",
        "- Keep `'` (singular apostrophes) for contractions -- in the data, we see a lot of `n't`, `'ll`, etc. These can be important for context (ie. `did` followed by `n't` vs just having `did` alone)\n",
        "- Replace `/` (forward slashes) with ` ` (spaces) -- this is because there are 2 cases that forward slashes would be used:\n",
        "    - Between numbers in fractions, but this context can be implied using bigrams (eg. 1 followed by 3)\n",
        "    - Between words used as \"or\" (eg. clean/friendly), which isn't that important for context\n",
        "- Remove all other characters\n",
        "- Trim all remaining strings and filter out those that are empty\n"
      ],
      "metadata": {
        "id": "FC2eyodGc7uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip A1_DATASET.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Aa-hh3Nc-6u",
        "outputId": "834f7cf0-be4e-4951-e4d0-6426f9a25e5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  A1_DATASET.zip\n",
            "   creating: A1_DATASET/\n",
            "  inflating: A1_DATASET/.DS_Store    \n",
            "  inflating: __MACOSX/A1_DATASET/._.DS_Store  \n",
            "  inflating: A1_DATASET/train.txt    \n",
            "  inflating: __MACOSX/A1_DATASET/._train.txt  \n",
            "  inflating: A1_DATASET/val.txt      \n",
            "  inflating: __MACOSX/A1_DATASET/._val.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(path) -> list[str]:\n",
        "    \"\"\"Read in a file and return a list of lines.\"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "        lines = [line.rstrip() for line in f]\n",
        "    return lines\n",
        "\n",
        "input_train = read_file('A1_DATASET/train.txt')\n",
        "input_val = read_file('A1_DATASET/val.txt')\n",
        "input_train[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfcZrxkLdRmY",
        "outputId": "bdb185b8-4b8a-4432-bead-dcc371f22087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I booked two rooms four months in advance at the Talbott . We were placed on the top floor next to the elevators , which are used all night long . When speaking to the front desk , I was told that they were simply honoring my request for an upper floor , which I had requested for a better view . I am looking at a brick wall , and getting no sleep . He also told me that they had received complaints before from guests on the 16th floor , and were aware of the noise problem . Why then did they place us on this floor when the hotel is not totally booked ? A request for an upper floor does not constitute placing someone on the TOP floor and using that request to justify this . If you decide to stay here , request a room on a lower floor and away from the elevator ! I spoke at length when booking my two rooms about my preferences . This is simply poor treatment of a guest whom they believed would not complain .',\n",
              " \"I LOVED this hotel . The room was so chic and trendy , the bed was comfortable , great slippers and robes . I love the Keihl 's bath products in the bathroom . We went during my birthday weekend , and they had a card and plate with pastries waiting in the room . We got a great deal on a junior suite from Travelzoo , but when we tried to take another trip to Chicago , the deal was gone . I really recommend the hotel . I LOVED it . The ONLY problem we had was with the David Burke Steakhouse . The service was horrible , they switched our order with the table next to our 's and the only compensation was they took off a $ 7 side dish we ordered ( and they were nice enough to NOT charge us for the other side dish we DID N'T get ) . DO N'T go there . Go to the Lawry 's accross the street . But stay at the James . ( BTW- its not a 3-star hotel like the website lists- its at least 4-star )\",\n",
              " \"The Hard Rock Hotel Chicago has become my favorite hotel . I 've stayed there at least 5 times now and have never had anything other than a wonderful experience . As you might have guessed - it has a super Rock & Roll theme , with some music paraphanelia in the lobby and on each floor 's elevator lobby . The rooms all have large photo murals that are themed to different musicians . You can request being on the floor of your favorite band ( like KISS , Aerosmith , etc ... ) The rooms are GREAT . Well appointed . SUPER Comfortable beds and luxurious sheets and wonderous pillows . The large TV has a cool sound system that ramps up the viewing experience . There 's a spacious desk in each room with a mini bar . I 've always had a room with big windows and a view down Michigan Ave ( the hotel sits right on Michigan - a block from the canal ) . The bathrooms are furnished with absolutely great fixtures sporting a great design ... some of the showers have windows in side of them ( you can shower AND enjoy the view down Michigan ! ) or close the waterproof drapes . I 've never had anything but a great experience with the staff here ... front desk , doorman , etc are all great . There 's a lobby bar that still feels intimate and has nice energy . Big screen TV to catch the latest scores , too . Of course , Rock & Roll music pumps thru the place . You can stay at a `` plain '' hotel anywhere ... but this is a GREAT , WELL DESIGNED , FUN and MEMORABLE HOTEL that you 'll want to return back to . If I had one concern - it would be the rather crazy charge for parking your car overnight . My daily car parking rate was close to $ 50 per night , which I 'm sorry to say is about the going rate in the loop . Stay here . Be cool .\"]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_str(input: str) -> str:\n",
        "    \"\"\"Remove all but spaces and alpha\"\"\"\n",
        "    input = input.lower().replace('/', ' ')\n",
        "    return ''.join(char for char in input if char.isalpha() or char in \" \").strip()\n",
        "\n",
        "\n",
        "def preprocess(input: list[str]) -> list[list[str]]:\n",
        "    \"\"\"Preprocess the input data according to a set of rules.\"\"\"\n",
        "    input = map(clean_str, input)\n",
        "    return list(filter(lambda line: line != '', map(lambda line: line.split(), input)))"
      ],
      "metadata": {
        "id": "jneZzsfLgEM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = preprocess(input_train)\n",
        "val = preprocess(input_val)\n",
        "train[0][:20], val[0][:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q7RzN9fq8VC",
        "outputId": "64a3bb77-a62a-47a0-d604-64dd5f3b13f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['i',\n",
              "  'booked',\n",
              "  'two',\n",
              "  'rooms',\n",
              "  'four',\n",
              "  'months',\n",
              "  'in',\n",
              "  'advance',\n",
              "  'at',\n",
              "  'the',\n",
              "  'talbott',\n",
              "  'we',\n",
              "  'were',\n",
              "  'placed',\n",
              "  'on',\n",
              "  'the',\n",
              "  'top',\n",
              "  'floor',\n",
              "  'next',\n",
              "  'to'],\n",
              " ['i',\n",
              "  'stayed',\n",
              "  'for',\n",
              "  'four',\n",
              "  'nights',\n",
              "  'while',\n",
              "  'attending',\n",
              "  'a',\n",
              "  'conference',\n",
              "  'the',\n",
              "  'hotel',\n",
              "  'is',\n",
              "  'in',\n",
              "  'a',\n",
              "  'great',\n",
              "  'spot',\n",
              "  'easy',\n",
              "  'walk',\n",
              "  'to',\n",
              "  'michigan'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsmoothed N-grams\n",
        "\n",
        "We will calculate the unigram and bigram counts of the training set, where the unigram counts is stored in a dictionary with `{word: count}` and bigrams are stored in a dictionary with `{(word1, word2): count}`.\n",
        "\n",
        "Then, we'll use the formulas for calculating the unigram/bigram probability models."
      ],
      "metadata": {
        "id": "RVx4tQr3c1aa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwkzCFGnbxgG"
      },
      "outputs": [],
      "source": [
        "unigram_counts = {}\n",
        "bigram_counts = {}\n",
        "\n",
        "# Compute unigram counts\n",
        "for line in train:\n",
        "    for word in line:\n",
        "        if word in unigram_counts:\n",
        "            unigram_counts[word] += 1\n",
        "        else:\n",
        "            unigram_counts[word] = 1\n",
        "\n",
        "# Compute bigram counts\n",
        "for line in train:\n",
        "    for i in range(len(line) - 1):\n",
        "        bigram = (line[i], line[i + 1])\n",
        "        if bigram in bigram_counts:\n",
        "            bigram_counts[bigram] += 1\n",
        "        else:\n",
        "            bigram_counts[bigram] = 1\n",
        "\n",
        "# Compute unigram and bigrams probabilities model\n",
        "# For unigrams, it's simply the count of word / total number of words\n",
        "# For bigrams, it's Count(A, B) / Count(A)\n",
        "unigram_probs = {word: count / sum(unigram_counts.values()) for word, count in unigram_counts.items()}\n",
        "bigram_probs = {bigram: count / unigram_counts[bigram[0]] for bigram, count in bigram_counts.items()}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(unigram_probs['the'])\n",
        "print(bigram_probs[('the', 'hotel')])\n",
        "\n",
        "# Print top 10 probable words in both models\n",
        "print(sorted(unigram_probs.items(), key=lambda x: x[1], reverse=True)[:10])\n",
        "print(sorted(bigram_probs.items(), key=lambda x: x[1], reverse=True)[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvfJndimlgzB",
        "outputId": "f460ad30-0d5b-4cda-f68a-9c47c5b74e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.06739116619002225\n",
            "0.07808374198415692\n",
            "[('the', 0.06739116619002225), ('and', 0.03298379408960915), ('a', 0.028573244359707657), ('to', 0.026564982523037815), ('was', 0.023209405783285668), ('i', 0.021773117254528122), ('in', 0.01604067365745154), ('we', 0.014197648554178583), ('of', 0.013295201779472514), ('hotel', 0.013180807117890055)]\n",
            "[(('honoring', 'my'), 1.0), (('constitute', 'placing'), 1.0), (('placing', 'someone'), 1.0), (('justify', 'this'), 1.0), (('decide', 'to'), 1.0), (('preferences', 'this'), 1.0), (('believed', 'would'), 1.0), (('keihl', 's'), 1.0), (('junior', 'suite'), 1.0), (('lawry', 's'), 1.0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Smoothing\n",
        "\n",
        "To handle unknown words, we replace them with the `<UNKNOWN>` keyword.\n",
        "\n",
        "We will apply Laplace Smoothing (takes ~52 secs). Additionally, we will apply +2 Smoothing."
      ],
      "metadata": {
        "id": "c6S3YGbhj7UL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add <UNKNOWN> keyword\n",
        "unigram_counts['<UNKNOWN>'] = 0\n",
        "bigram_counts[('<UNKNOWN>', '<UNKNOWN>')] = 0"
      ],
      "metadata": {
        "id": "5P3M1-XIn7ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate list of bigram probabilities with Laplace Smoothing\n",
        "bigram_probs_smoothed = {}\n",
        "\n",
        "unigram_list = list(unigram_counts.keys())\n",
        "\n",
        "for v in unigram_list:\n",
        "    for k in unigram_list:\n",
        "        bigram_probs_smoothed[(v, k)] = (bigram_counts.get((v, k), 0) + 1) / (unigram_counts.get(v, 0) + len(unigram_counts))"
      ],
      "metadata": {
        "id": "acokRncOjt2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate list of bigram probabilities with +2 Smoothing\n",
        "bigram_probs_smoothed_2 = {}\n",
        "\n",
        "unigram_list = list(unigram_counts.keys())\n",
        "\n",
        "for v in unigram_list:\n",
        "    for k in unigram_list:\n",
        "        bigram_probs_smoothed_2[(v, k)] = (bigram_counts.get((v, k), 0) + 2) / (unigram_counts.get(v, 0) + 2 * len(unigram_counts))"
      ],
      "metadata": {
        "id": "6yJCtC5PqNUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bigram_probs_smoothed[('the', 'hotel')])\n",
        "print(bigram_probs_smoothed_2[('the', 'hotel')])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXbuxG8soG8o",
        "outputId": "af7c5610-13de-40fc-a10c-38f1d1c20cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.03714311286136221\n",
            "0.024407416099507157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perplexity Calculation"
      ],
      "metadata": {
        "id": "En2MW8M4mgFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_perplexity(probabilities, validation_set, is_bigram=False):\n",
        "    total_log_prob = 0\n",
        "    token_count = 0\n",
        "\n",
        "    for line in validation_set:\n",
        "        for i, word in enumerate(line):\n",
        "            if is_bigram and i > 0:\n",
        "                # For bigram model\n",
        "                prev_word = line[i-1]\n",
        "                prob = probabilities.get((prev_word, word), probabilities.get(('<UNKNOWN>', '<UNKNOWN>'), 1e-10))\n",
        "            else:\n",
        "                # For unigram model\n",
        "                prob = probabilities.get(word, probabilities.get('<UNKNOWN>', 1e-10))\n",
        "\n",
        "            total_log_prob += -math.log(prob)\n",
        "            token_count += 1\n",
        "\n",
        "    average_log_prob = total_log_prob / token_count\n",
        "    perplexity = math.exp(average_log_prob)\n",
        "    return perplexity\n",
        "\n",
        "# Calculate perplexity for unigram model\n",
        "unigram_perplexity = calculate_perplexity(unigram_probs, val)\n",
        "\n",
        "# Calculate perplexity for bigram model\n",
        "bigram_perplexity = calculate_perplexity(bigram_probs, val, is_bigram=True)\n",
        "\n",
        "# Calculate perplexity for Laplace-smoothed bigram model\n",
        "bigram_smoothed_perplexity = calculate_perplexity(bigram_probs_smoothed, val, is_bigram=True)\n",
        "\n",
        "# Calculate perplexity for Add-2-smoothed bigram model\n",
        "bigram_smoothed2_perplexity = calculate_perplexity(bigram_probs_smoothed_2, val, is_bigram=True)\n",
        "\n",
        "\n",
        "print(f\"Unigram Model Perplexity: {unigram_perplexity}\")\n",
        "print(f\"Bigram Model Perplexity: {bigram_perplexity}\")\n",
        "print(f\"Bigram (Laplace Smoothing) Model Perplexity: {bigram_smoothed_perplexity}\")\n",
        "print(f\"Bigram (+2 Smoothing) Model Perplexity: {bigram_smoothed2_perplexity}\")"
      ],
      "metadata": {
        "id": "Wv1WdyC0qbAT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b203effa-da57-499e-e36b-ec6e826d506b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Model Perplexity: 729.0068112574407\n",
            "Bigram Model Perplexity: 30355.48594886052\n",
            "Bigram (Laplace Smoothing) Model Perplexity: 1384.1037358070637\n",
            "Bigram (+2 Smoothing) Model Perplexity: 1876.745092369044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pcVT5hP9x93D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}