{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "\n",
        "This is the full model, with all functions written inside the main class function. Detailed explaination of steps will be listed below in separate sections to run in individual Jupyter Notebook cells."
      ],
      "metadata": {
        "id": "r8bfgexGyC80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "class NGramLanguageModel:\n",
        "    def __init__(self, n=2):\n",
        "        self.n = n\n",
        "        self.ngram_counts = defaultdict(int)\n",
        "        self.context_counts = defaultdict(int)\n",
        "        self.vocab = set()\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        # Preserve 'US' by replacing it with a placeholder before any transformation\n",
        "        input = re.sub(r'\\bUS\\b', 'US_placeholder', text)\n",
        "\n",
        "        # Convert input to lowercase but preserve \"US\"\n",
        "        input_lower = input.lower()\n",
        "\n",
        "        # Handle prices to preserve the number before '/'\n",
        "        input_lower = re.sub(r'\\b(\\d+)/([a-zA-Z]+)', r'\\1 per \\2', input_lower)  # Separate number from '/night' or similar, replace / to per\n",
        "\n",
        "        # Simplify URLs to a placeholder\n",
        "        input_lower = re.sub(r'//[^ ]+', 'URL', input_lower)\n",
        "\n",
        "        # Split phrases like attractions/shopping into separate words\n",
        "        input_lower = re.sub(r'(\\D+)/(\\D+)', r'\\1 \\2', input_lower)  # Tokenize phrases separated by '/'\n",
        "\n",
        "        # Now replace 'us' with lowercase and handle special cases\n",
        "        input_lower = re.sub(r'\\bus\\b', 'us', input_lower)  # Ensure 'us' remains as 'us'\n",
        "\n",
        "        # Replace dates with the token 'calendardate'\n",
        "        input_lower = re.sub(r'\\b(\\d{1,2})/(\\d{1,2})/(\\d{2,4})\\b', 'calendardate', input_lower)  # Match dates like 9/5/2009\n",
        "\n",
        "        # Restore 'US' from the placeholder\n",
        "        input_lower = input_lower.replace('us_placeholder', 'US')\n",
        "\n",
        "        # Replace n't, 'll, 's, 're, 've\n",
        "        input_lower = input_lower.replace('n\\'t', 'not')\n",
        "        input_lower = input_lower.replace('\\'ll', 'will')\n",
        "        input_lower = input_lower.replace('\\'s', 'is')\n",
        "        input_lower = input_lower.replace('\\'re', 'are')\n",
        "        input_lower = input_lower.replace('\\'ve', 'have')\n",
        "\n",
        "        # Remove special characters but keep whitespace and alpha\n",
        "        cleaned = ''.join(char for char in input_lower if char.isalpha() or char in \"' /\")  # Space included to separate words\n",
        "\n",
        "        return cleaned.strip().split()\n",
        "\n",
        "    def train(self, corpus):\n",
        "        for line in corpus:\n",
        "            tokens = self.preprocess(line)\n",
        "            self.vocab.update(tokens)\n",
        "\n",
        "            # Loop through each character and loop through previous characters for n > 1\n",
        "            for i in range(len(tokens)):\n",
        "                for j in range(1, self.n + 1):\n",
        "                    if i + j <= len(tokens):    # Prevent overflow\n",
        "                        ngram = tuple(tokens[i:i+j])\n",
        "                        self.ngram_counts[ngram] += 1\n",
        "                        if j > 1:\n",
        "                            self.context_counts[ngram[:-1]] += 1\n",
        "\n",
        "    def unsmoothed_probability(self, ngram):\n",
        "        if len(ngram) == 1:\n",
        "            total_count = sum(self.ngram_counts[t] for t in self.ngram_counts if len(t) == 1)\n",
        "            return self.ngram_counts[ngram] / total_count if total_count > 0 else 0\n",
        "        else:\n",
        "            context = ngram[:-1]\n",
        "            return self.ngram_counts[ngram] / self.context_counts[context] if self.context_counts[context] > 0 else 0\n",
        "\n",
        "    def add_k_smoothing(self, ngram, k=0.1):\n",
        "        if len(ngram) == 1:\n",
        "            return (self.ngram_counts[ngram] + k) / (sum(self.ngram_counts[t] for t in self.ngram_counts if len(t) == 1) + k * len(self.vocab))\n",
        "        else:\n",
        "            context = ngram[:-1]\n",
        "            return (self.ngram_counts[ngram] + k) / (self.context_counts[context] + k * len(self.vocab))\n",
        "\n",
        "    def laplace_smoothing(self, ngram):\n",
        "        return self.add_k_smoothing(ngram, k=1)\n",
        "\n",
        "    def handle_unknown_words(self, word):\n",
        "        return \"<UNK>\" if word not in self.vocab else word\n",
        "\n",
        "    def perplexity(self, test_corpus, smoothing_method='', k=1):\n",
        "        log_prob_sum = 0\n",
        "        token_count = 0\n",
        "\n",
        "        for line in test_corpus:\n",
        "            tokens = self.preprocess(line)\n",
        "            tokens = [self.handle_unknown_words(token) for token in tokens]\n",
        "            token_count += len(tokens)\n",
        "\n",
        "            for i in range(len(tokens)):\n",
        "                context = tuple(tokens[max(0, i-self.n+1):i])\n",
        "                ngram = context + (tokens[i],)\n",
        "\n",
        "                if smoothing_method == 'laplace':\n",
        "                    prob = self.laplace_smoothing(ngram)\n",
        "                elif smoothing_method == 'add_k':\n",
        "                    prob = self.add_k_smoothing(ngram, k)\n",
        "                else:\n",
        "                    prob = self.unsmoothed_probability(ngram)\n",
        "\n",
        "                if prob > 0:\n",
        "                    log_prob_sum += math.log(prob)\n",
        "                else:\n",
        "                    # Handle zero probability by assigning a very low log probability\n",
        "                    log_prob_sum += math.log(1e-10)\n",
        "\n",
        "        return math.exp(-log_prob_sum / token_count)"
      ],
      "metadata": {
        "id": "uKJ1BwR5yL5s"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading/Preprocessing\n",
        "\n",
        "Upload the `A1_DATASET.zip` file from eLearning to the files.\n",
        "\n"
      ],
      "metadata": {
        "id": "FC2eyodGc7uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o A1_DATASET.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44wUwHgWyFGD",
        "outputId": "1a0be85b-b734-43ca-fa0a-c1f82f3ddf71"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  A1_DATASET.zip\n",
            "  inflating: A1_DATASET/.DS_Store    \n",
            "  inflating: __MACOSX/A1_DATASET/._.DS_Store  \n",
            "  inflating: A1_DATASET/train.txt    \n",
            "  inflating: __MACOSX/A1_DATASET/._train.txt  \n",
            "  inflating: A1_DATASET/val.txt      \n",
            "  inflating: __MACOSX/A1_DATASET/._val.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training data\n",
        "with open('A1_DATASET/train.txt', 'r', encoding='utf-8') as f:\n",
        "    train_corpus = f.readlines()\n",
        "\n",
        "# Load validation data\n",
        "with open('A1_DATASET/val.txt', 'r', encoding='utf-8') as f:\n",
        "    validation_corpus = f.readlines()"
      ],
      "metadata": {
        "id": "hfo-6Wbb7rY6"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Explaination\n",
        "\n",
        "- Keep alphabetic characters (convert all to lowercase) but keep `US` still `US city` rather than `us city`\n",
        "- Process `'` (singular apostrophes) for contractions -- in the data, we see a lot of `n't`, `'ll`, `'d` etc. These can be important for context.\n",
        "  - Replace `n't` to `not`, `'ll` to `will`, `'re` to `are`, `'ve` to `have` but keep `'d` unchanged since `'d`can not only be `would`, but also `had`, if not consider the whole sentence and contex, hard to decide.\n",
        "- Process `/` (forward slashes) -- this is because there are 4 cases that forward slashes would be used:\n",
        "    - Between numbers in fractions, like `1/2`\n",
        "    - Between words used as \"or\" (eg. `clean/friendly`, `Housekeeping/Turn Down Service`), alternative words\n",
        "    - Date, like `11/20`, or `11/13/2010`\n",
        "    - Price, like `$ 58/night`\n",
        "- Remove all other characters\n",
        "- Trim all remaining strings and filter out those that are empty\n",
        "\n",
        "This pre-processing step will happen on each line of the dataset, whenever training or validation happens.\n",
        "\n",
        "## Training\n",
        "\n",
        "The \"training\" of this model is really just getting the counts of ngrams, as well as the counts of the respective \"contexts\" of the ngrams. For unigrams, this isn't needed as we simply divide by the total number of unique terms, but for unigrams we do need to keep track of the unigram prior, which is the context of the bigram in this case."
      ],
      "metadata": {
        "id": "bVFFKfcox_kI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train unigram model\n",
        "unigram_model = NGramLanguageModel(n=1)\n",
        "unigram_model.train(train_corpus)\n",
        "\n",
        "# Train bigram model\n",
        "bigram_model = NGramLanguageModel(n=2)\n",
        "bigram_model.train(train_corpus)"
      ],
      "metadata": {
        "id": "_PzfszQu8CG_"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsmoothed N-grams\n",
        "\n",
        "We have calculated the unigram and bigram counts of the training set, stored in their respective counts array. We will then use these counts to calculate the perplexity of the model, without doing any smoothing. For all unknown words, we assign a very low log-probability so we don't end up with infinities when taking the log."
      ],
      "metadata": {
        "id": "RVx4tQr3c1aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate perplexity for unigram model\n",
        "unigram_perplexity = unigram_model.perplexity(validation_corpus)\n",
        "print(f\"Unsmoothed Unigram model perplexity: {unigram_perplexity}\")\n",
        "\n",
        "# Calculate perplexity for bigram model\n",
        "bigram_perplexity = bigram_model.perplexity(validation_corpus)\n",
        "print(f\"Unsmoothed Bigram model perplexity: {bigram_perplexity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbeZLGSt9ahe",
        "outputId": "07e2d593-0bd4-4ed8-e1ae-d00c8e50d007"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsmoothed Unigram model perplexity: 720.5399781710248\n",
            "Unsmoothed Bigram model perplexity: 25679.71686225255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Smoothing\n",
        "\n",
        "To handle unknown words, we replace them with the `<UNKNOWN>` keyword. To perform smoothing, we use Laplace Smoothing as well as a series of add-k smoothing, with differing values of k."
      ],
      "metadata": {
        "id": "c6S3YGbhj7UL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate perplexity for unigram model with laplace smoothing\n",
        "unigram_laplace_perplexity = unigram_model.perplexity(validation_corpus, smoothing_method='laplace')\n",
        "print(f\"Unigram model perplexity with Laplace smoothing: {unigram_laplace_perplexity}\")\n",
        "\n",
        "# Calculate perplexity for bigram model with laplace smoothing\n",
        "bigram_laplace_perplexity = bigram_model.perplexity(validation_corpus, smoothing_method='laplace')\n",
        "print(f\"Bigram model perplexity with Laplace smoothing: {bigram_laplace_perplexity}\")\n",
        "\n",
        "# Calculate perplexity for unigram model with add-k smoothing (of different k values\n",
        "unigram_add_k_perplexity = unigram_model.perplexity(validation_corpus, smoothing_method='add_k', k=0.1)\n",
        "print(f\"Unigram model perplexity with add-k (k=0.1) smoothing: {unigram_add_k_perplexity}\")\n",
        "\n",
        "unigram_add_k_perplexity = unigram_model.perplexity(validation_corpus, smoothing_method='add_k', k=0.05)\n",
        "print(f\"Unigram model perplexity with add-k (k=0.05) smoothing: {unigram_add_k_perplexity}\")\n",
        "\n",
        "# Calculate perplexity for bigram model with add-k smoothing (of different k values\n",
        "bigram_add_k_perplexity = bigram_model.perplexity(validation_corpus, smoothing_method='add_k', k=0.1)\n",
        "print(f\"Bigram model perplexity with add-k (k=0.1) smoothing: {bigram_add_k_perplexity}\")\n",
        "\n",
        "bigram_add_k_perplexity = bigram_model.perplexity(validation_corpus, smoothing_method='add_k', k=0.05)\n",
        "print(f\"Bigram model perplexity with add-k (k=0.05) smoothing: {bigram_add_k_perplexity}\")\n",
        "\n",
        "bigram_add_k_perplexity = bigram_model.perplexity(validation_corpus, smoothing_method='add_k', k=0.01)\n",
        "print(f\"Bigram model perplexity with add-k (k=0.01) smoothing: {bigram_add_k_perplexity}\")\n",
        "\n",
        "bigram_add_k_perplexity = bigram_model.perplexity(validation_corpus, smoothing_method='add_k', k=0.005)\n",
        "print(f\"Bigram model perplexity with add-k (k=0.005) smoothing: {bigram_add_k_perplexity}\")\n",
        "\n",
        "bigram_add_k_perplexity = bigram_model.perplexity(validation_corpus, smoothing_method='add_k', k=0.001)\n",
        "print(f\"Bigram model perplexity with add-k (k=0.001) smoothing: {bigram_add_k_perplexity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ne7u1Jx_vbE",
        "outputId": "8cbdd16f-9fde-4b13-82fd-62067baa3fbb"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram model perplexity with Laplace smoothing: 513.1057379235663\n",
            "Bigram model perplexity with Laplace smoothing: 1189.9708957449484\n",
            "Unigram model perplexity with add-k (k=0.1) smoothing: 537.4355160606028\n",
            "Unigram model perplexity with add-k (k=0.05) smoothing: 548.5980688072752\n",
            "Bigram model perplexity with add-k (k=0.1) smoothing: 489.42475249933585\n",
            "Bigram model perplexity with add-k (k=0.05) smoothing: 413.19023790252487\n",
            "Bigram model perplexity with add-k (k=0.01) smoothing: 348.5153193289591\n",
            "Bigram model perplexity with add-k (k=0.005) smoothing: 354.9874579340956\n",
            "Bigram model perplexity with add-k (k=0.001) smoothing: 440.4381085098001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fKtxv1lPAZFW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}